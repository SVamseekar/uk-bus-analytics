# Technical Implementation Plan
## UK Bus Transport Intelligence Platform

**Document Type:** System Design & Implementation Specification
**Target Audience:** Engineering Team
**Version:** 1.0
**Date:** October 2025
**Status:** Implementation-Ready

---

## 1. System Overview

The UK Bus Transport Intelligence Platform is a cloud-deployed analytics system that processes multi-source transport and demographic data to provide real-time insights, economic appraisal, and policy simulation capabilities. The system ingests data from government APIs (BODS, ONS, NOMIS), processes it through an automated ETL pipeline, applies machine learning models for insight generation, and exposes results through an interactive Streamlit dashboard with embedded AI advisory capabilities.

**Core Technical Challenge:** Process 400k+ bus stops and 35k+ routes monthly, aggregate to 7,696 LSOA-level metrics, run ML inference and economic calculations, and deliver sub-second query performance in a <1GB deployment footprint.

---

## 2. Technology Stack Overview

### 2.1 Core Languages & Frameworks

| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| **Primary Language** | Python | 3.11+ | All data processing, ML, and dashboard logic |
| **Dashboard Framework** | Streamlit | 1.28+ | Interactive web interface |
| **Data Processing** | Pandas | 2.1+ | Dataframe operations |
| **Query Engine** | DuckDB | 0.9+ | In-memory SQL analytics |
| **ML Framework** | Scikit-learn | 1.3+ | Clustering, classification, anomaly detection |
| **NLP Models** | Sentence Transformers | 2.2+ | Text embeddings for semantic search |
| **Visualization** | Plotly | 5.17+ | Interactive charts |
| **Mapping** | Folium | 0.15+ | Interactive geospatial visualizations |

### 2.2 Data Storage & Serialization

| Technology | Purpose | Format |
|-----------|---------|--------|
| **Parquet** | Primary storage format | Columnar, compressed (Snappy) |
| **JSON** | Pre-computed analysis results | Structured outputs |
| **Pickle** | ML model serialization | Trained model persistence |
| **CSV** | Legacy compatibility | Raw data exchange |

### 2.3 ML & Analytics Libraries

```python
# requirements.txt (key dependencies)
pandas==2.1.3
numpy==1.26.2
scikit-learn==1.3.2
sentence-transformers==2.2.2
hdbscan==0.8.33
plotly==5.17.0
folium==0.15.0
streamlit==1.28.1
duckdb==0.9.2
pyarrow==14.0.1  # Parquet support
```

### 2.4 Infrastructure & Deployment

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Hosting** | Hugging Face Spaces | Free tier cloud deployment |
| **CI/CD** | GitHub Actions | Automated testing and deployment |
| **Version Control** | Git/GitHub | Source code management |
| **Environment** | Docker (optional) | Containerized deployment |
| **Monitoring** | Streamlit built-in + custom logging | Performance tracking |

---

## 3. System Architecture & Data Flow

### 3.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACE                            â”‚
â”‚  Streamlit Dashboard (6 Pages) + AI Chat Interface              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   APPLICATION LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Query Engine â”‚  â”‚  ML Inferenceâ”‚  â”‚ BCR Calculatorâ”‚          â”‚
â”‚  â”‚  (DuckDB)    â”‚  â”‚   (Sklearn)  â”‚  â”‚   (Custom)   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA LAYER                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  lsoa_metrics.parquet (7,696 rows, ~50MB)                â”‚  â”‚
â”‚  â”‚  - Service metrics (stops, routes, frequency)            â”‚  â”‚
â”‚  â”‚  - Demographics (population, IMD, unemployment)          â”‚  â”‚
â”‚  â”‚  - Derived features (stops_per_capita, equity_score)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  precomputed_insights.json (~20MB)                        â”‚  â”‚
â”‚  â”‚  - Coverage analysis, equity metrics, BCR results        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ml_models/ (pickled models, ~100MB)                     â”‚  â”‚
â”‚  â”‚  - route_embeddings.pkl, anomaly_detector.pkl           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ETL PIPELINE (Offline)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Data Ingest  â”‚â†’ â”‚ Integration  â”‚â†’ â”‚  Aggregation â”‚          â”‚
â”‚  â”‚ (BODS, ONS)  â”‚  â”‚ (Join/Enrich)â”‚  â”‚  (LSOA-level)â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ ML Training  â”‚  â”‚ BCR Compute  â”‚  â”‚ Export Data  â”‚          â”‚
â”‚  â”‚ (Models)     â”‚  â”‚ (Economics)  â”‚  â”‚ (Parquet)    â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DATA SOURCES (APIs)                             â”‚
â”‚  - Bus Open Data Service (BODS)                                 â”‚
â”‚  - ONS Census & Mid-Year Estimates                              â”‚
â”‚  - NOMIS Labour Market Statistics                               â”‚
â”‚  - IMD Deprivation Data (GOV.UK)                               â”‚
â”‚  - Schools Data (GIAS)                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Data Flow Sequence

```
1. INGESTION (Monthly, Automated)
   â”œâ”€ Download GTFS feeds from BODS
   â”œâ”€ Fetch demographic data from ONS API
   â”œâ”€ Fetch employment data from NOMIS API
   â””â”€ Validate data quality

2. INTEGRATION (Offline Processing)
   â”œâ”€ Parse GTFS files (stops.txt, routes.txt, trips.txt, shapes.txt)
   â”œâ”€ Geocode stops to LSOA boundaries
   â”œâ”€ Join demographics to LSOAs
   â””â”€ Calculate service metrics (stops/routes per LSOA)

3. AGGREGATION (Data Reduction)
   â”œâ”€ Aggregate 400k stops â†’ 7,696 LSOA metrics
   â”œâ”€ Compute derived features (stops_per_capita, equity_index)
   â”œâ”€ Generate regional summaries (9 regions)
   â””â”€ Export to Parquet (~50MB compressed)

4. ML PROCESSING (Model Training/Inference)
   â”œâ”€ Train route clustering (Sentence Transformers + HDBSCAN)
   â”œâ”€ Train anomaly detector (Isolation Forest)
   â”œâ”€ Train coverage predictor (Random Forest)
   â””â”€ Serialize models to pickle files

5. ECONOMIC ANALYSIS (Pre-computation)
   â”œâ”€ Calculate BCR for top 100 investment scenarios
   â”œâ”€ Compute GDP multipliers by region
   â”œâ”€ Estimate employment impacts
   â””â”€ Export results to JSON

6. DEPLOYMENT (Dashboard Runtime)
   â”œâ”€ Load lsoa_metrics.parquet into DuckDB
   â”œâ”€ Load precomputed_insights.json
   â”œâ”€ Load ML models (lazy loading)
   â””â”€ Serve Streamlit dashboard
```

---

## 4. Module-Level Implementation

### 4.1 Module 1: Service Coverage & Accessibility Intelligence

**Dashboard Page:** `01_ðŸ—ºï¸_Service_Coverage.py`

#### 4.1.1 Data Requirements

**Input Schema:**
```python
# lsoa_metrics.parquet columns used
{
    'lsoa_code': str,           # E.g., 'E01000001'
    'lsoa_name': str,           # E.g., 'City of London 001A'
    'region': str,              # E.g., 'London'
    'bus_stops_count': int,     # Number of stops in LSOA
    'routes_count': int,        # Number of unique routes
    'population': int,          # Census 2021
    'area_sq_km': float,        # LSOA area
    'imd_score': float,         # 0-100 deprivation score
    'imd_decile': int,          # 1 (most deprived) - 10 (least)
    'latitude': float,          # LSOA centroid
    'longitude': float,
}
```

#### 4.1.2 Key Metrics Computed

```python
# analysis/spatial/coverage_metrics.py

def compute_coverage_metrics(lsoa_data: pd.DataFrame) -> dict:
    """
    Compute service coverage metrics

    Returns:
        {
            'stops_per_capita': Series,      # Stops per 1,000 residents
            'stops_per_sq_km': Series,       # Stop density
            'coverage_score': Series,        # 0-100 composite score
            'service_gaps': DataFrame,       # LSOAs below threshold
            'regional_summary': DataFrame    # Aggregated by region
        }
    """

    # Metric 1: Stops per capita
    lsoa_data['stops_per_1k_pop'] = (
        lsoa_data['bus_stops_count'] / lsoa_data['population'] * 1000
    )

    # Metric 2: Stop density
    lsoa_data['stops_per_sq_km'] = (
        lsoa_data['bus_stops_count'] / lsoa_data['area_sq_km']
    )

    # Metric 3: Coverage score (composite index)
    # Normalize metrics to 0-100 scale
    stops_per_capita_norm = (
        (lsoa_data['stops_per_1k_pop'] - lsoa_data['stops_per_1k_pop'].min()) /
        (lsoa_data['stops_per_1k_pop'].max() - lsoa_data['stops_per_1k_pop'].min()) * 100
    )

    routes_per_capita_norm = (
        (lsoa_data['routes_count'] / lsoa_data['population'] * 100000)
    )
    routes_per_capita_norm = (
        (routes_per_capita_norm - routes_per_capita_norm.min()) /
        (routes_per_capita_norm.max() - routes_per_capita_norm.min()) * 100
    )

    # Composite score (weighted average)
    lsoa_data['coverage_score'] = (
        stops_per_capita_norm * 0.6 +  # 60% weight on stops
        routes_per_capita_norm * 0.4   # 40% weight on routes
    )

    # Identify service gaps (bottom 10% coverage)
    threshold = lsoa_data['coverage_score'].quantile(0.10)
    service_gaps = lsoa_data[lsoa_data['coverage_score'] < threshold].copy()

    # Regional aggregation
    regional_summary = lsoa_data.groupby('region').agg({
        'bus_stops_count': 'sum',
        'routes_count': 'sum',
        'population': 'sum',
        'coverage_score': 'mean'
    }).reset_index()

    regional_summary['stops_per_1k_pop'] = (
        regional_summary['bus_stops_count'] / regional_summary['population'] * 1000
    )

    return {
        'stops_per_capita': lsoa_data['stops_per_1k_pop'],
        'stops_per_sq_km': lsoa_data['stops_per_sq_km'],
        'coverage_score': lsoa_data['coverage_score'],
        'service_gaps': service_gaps,
        'regional_summary': regional_summary
    }
```

#### 4.1.3 Visualization Components

```python
# dashboard/pages/01_ðŸ—ºï¸_Service_Coverage.py

import streamlit as st
import plotly.express as px
import folium
from streamlit_folium import folium_static

def render_coverage_page():
    st.title("ðŸ—ºï¸ Service Coverage & Accessibility")

    # Load data
    lsoa_data = load_lsoa_data()
    coverage_metrics = compute_coverage_metrics(lsoa_data)

    # KPI Cards (top row)
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Bus Stops", f"{lsoa_data['bus_stops_count'].sum():,}")
    with col2:
        avg_coverage = coverage_metrics['coverage_score'].mean()
        st.metric("Avg Coverage Score", f"{avg_coverage:.1f}/100")
    with col3:
        service_gaps_count = len(coverage_metrics['service_gaps'])
        st.metric("Service Gap LSOAs", f"{service_gaps_count:,}")
    with col4:
        underserved_pop = coverage_metrics['service_gaps']['population'].sum()
        st.metric("Underserved Population", f"{underserved_pop:,}")

    # Interactive Map
    st.subheader("Interactive Coverage Map")

    # Folium map with choropleth
    m = folium.Map(location=[54.5, -3.5], zoom_start=6)

    folium.Choropleth(
        geo_data='data/geojson/lsoa_boundaries.json',  # GeoJSON boundaries
        name='coverage',
        data=lsoa_data,
        columns=['lsoa_code', 'coverage_score'],
        key_on='feature.properties.lsoa_code',
        fill_color='RdYlGn',  # Red (low) â†’ Yellow â†’ Green (high)
        fill_opacity=0.7,
        line_opacity=0.2,
        legend_name='Coverage Score (0-100)'
    ).add_to(m)

    folium_static(m, width=1200, height=600)

    # Regional Comparison Chart
    st.subheader("Regional Coverage Comparison")

    fig = px.bar(
        coverage_metrics['regional_summary'],
        x='region',
        y='stops_per_1k_pop',
        color='coverage_score',
        title='Stops per 1,000 Residents by Region',
        labels={'stops_per_1k_pop': 'Stops per 1k Population'},
        color_continuous_scale='RdYlGn'
    )
    st.plotly_chart(fig, use_container_width=True)

    # Service Gaps Table
    st.subheader("Service Gap Areas (Bottom 10%)")

    gaps_display = coverage_metrics['service_gaps'][[
        'lsoa_name', 'region', 'population', 'bus_stops_count',
        'coverage_score'
    ]].sort_values('coverage_score')

    st.dataframe(
        gaps_display.head(50),
        use_container_width=True,
        column_config={
            'coverage_score': st.column_config.ProgressColumn(
                'Coverage Score',
                format="%.1f",
                min_value=0,
                max_value=100
            )
        }
    )
```

---

### 4.2 Module 2: Network Optimization Intelligence

**Dashboard Page:** `02_ðŸšŒ_Network_Optimization.py`

#### 4.2.1 ML Model: Route Clustering

**Objective:** Identify similar routes for potential consolidation

**Algorithm:** Sentence Transformers (text embeddings) + HDBSCAN (clustering)

**Implementation:**

```python
# analysis/spatial/02_train_ml_models.py

from sentence_transformers import SentenceTransformer
import hdbscan
import numpy as np
import pickle

def train_route_clustering_model(routes_data: pd.DataFrame):
    """
    Train route clustering model using semantic embeddings

    Input Schema:
        - route_id: str
        - route_name: str (e.g., "X1 Manchester - Bolton")
        - operator: str
        - stop_sequence: str (comma-separated stop names)

    Output:
        - route_clusters.pkl (model + cluster assignments)
    """

    # Step 1: Create route descriptions
    routes_data['description'] = routes_data.apply(
        lambda r: f"Route {r['route_name']} operated by {r['operator']}. "
                  f"Stops: {r['stop_sequence'][:200]}",  # Truncate long sequences
        axis=1
    )

    # Step 2: Generate embeddings using pre-trained model
    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    embeddings = model.encode(
        routes_data['description'].tolist(),
        show_progress_bar=True,
        batch_size=32
    )

    # Step 3: Cluster embeddings using HDBSCAN
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=5,      # At least 5 routes per cluster
        min_samples=2,           # Flexible clustering
        metric='euclidean',
        cluster_selection_epsilon=0.5
    )

    cluster_labels = clusterer.fit_predict(embeddings)

    # Step 4: Analyze clusters
    routes_data['cluster'] = cluster_labels

    # Cluster statistics
    cluster_summary = routes_data[routes_data['cluster'] != -1].groupby('cluster').agg({
        'route_id': 'count',
        'operator': lambda x: ', '.join(x.unique()[:3]),  # Top 3 operators
        'route_name': lambda x: x.iloc[0]  # Representative route
    }).rename(columns={'route_id': 'num_routes'})

    # Step 5: Save model
    model_artifacts = {
        'embeddings_model': model,
        'clusterer': clusterer,
        'embeddings': embeddings,
        'cluster_labels': cluster_labels,
        'cluster_summary': cluster_summary
    }

    with open('models/route_clusters.pkl', 'wb') as f:
        pickle.dump(model_artifacts, f)

    print(f"âœ… Route clustering complete")
    print(f"   Total routes: {len(routes_data)}")
    print(f"   Clusters found: {len(cluster_summary)}")
    print(f"   Outliers: {(cluster_labels == -1).sum()}")

    return model_artifacts
```

**Usage in Dashboard:**

```python
# dashboard/pages/02_ðŸšŒ_Network_Optimization.py

def load_route_clusters():
    with open('models/route_clusters.pkl', 'rb') as f:
        return pickle.load(f)

def render_network_optimization_page():
    st.title("ðŸšŒ Network Optimization Intelligence")

    # Load clusters
    clusters = load_route_clusters()
    cluster_summary = clusters['cluster_summary']

    # Visualization: Cluster sizes
    st.subheader("Route Overlap Opportunities")

    fig = px.bar(
        cluster_summary.reset_index(),
        x='cluster',
        y='num_routes',
        title='Number of Similar Routes per Cluster',
        labels={'num_routes': 'Routes in Cluster', 'cluster': 'Cluster ID'}
    )
    st.plotly_chart(fig)

    # Interactive cluster explorer
    st.subheader("Explore Route Clusters")

    selected_cluster = st.selectbox(
        "Select a cluster to view similar routes",
        cluster_summary.index
    )

    # Show routes in selected cluster
    routes_in_cluster = routes_data[routes_data['cluster'] == selected_cluster]
    st.dataframe(routes_in_cluster[['route_name', 'operator', 'stop_sequence']])

    st.info(f"ðŸ’¡ **Optimization Opportunity:** These {len(routes_in_cluster)} routes "
            f"have similar coverage patterns. Consider coordination or consolidation.")
```

---

### 4.3 Module 3: Equity Intelligence

**Dashboard Page:** `03_âš–ï¸_Equity_Intelligence.py`

#### 4.3.1 Equity Index Calculation

```python
# analysis/spatial/equity_metrics.py

def compute_equity_index(lsoa_data: pd.DataFrame) -> pd.Series:
    """
    Multi-dimensional equity index (0-100 scale)

    Higher score = better equity (service matches need)

    Dimensions:
        1. Deprivation-adjusted coverage (40% weight)
        2. Age-adjusted accessibility (30% weight)
        3. Car ownership vs. service level (30% weight)
    """

    # Dimension 1: Deprivation-adjusted coverage
    # Expected: High deprivation â†’ High service (inverse relationship)
    deprivation_need_score = (10 - lsoa_data['imd_decile']) / 10 * 100  # Invert decile
    actual_coverage_score = lsoa_data['coverage_score']

    # Equity = how well actual coverage matches need
    deprivation_equity = 100 - abs(deprivation_need_score - actual_coverage_score)

    # Dimension 2: Age-adjusted accessibility
    # Expected: High elderly% â†’ High service
    elderly_need_score = lsoa_data['elderly_pct'] * 100  # Normalize to 0-100
    age_equity = 100 - abs(elderly_need_score - actual_coverage_score)

    # Dimension 3: Car ownership equity
    # Expected: Low car ownership â†’ High service
    car_need_score = (1 - lsoa_data['car_ownership_rate']) * 100
    car_equity = 100 - abs(car_need_score - actual_coverage_score)

    # Composite equity index (weighted)
    equity_index = (
        deprivation_equity * 0.40 +
        age_equity * 0.30 +
        car_equity * 0.30
    )

    return equity_index


def identify_equity_gaps(lsoa_data: pd.DataFrame) -> pd.DataFrame:
    """
    Identify LSOAs with poor equity (low score + high need)
    """

    lsoa_data['equity_index'] = compute_equity_index(lsoa_data)

    # Priority areas: High need (IMD decile 1-3) + Low equity (bottom 25%)
    equity_threshold = lsoa_data['equity_index'].quantile(0.25)

    equity_gaps = lsoa_data[
        (lsoa_data['imd_decile'] <= 3) &
        (lsoa_data['equity_index'] < equity_threshold)
    ].copy()

    equity_gaps = equity_gaps.sort_values('equity_index')

    return equity_gaps
```

#### 4.3.2 Visualization: Equity Heatmap

```python
# dashboard/pages/03_âš–ï¸_Equity_Intelligence.py

def render_equity_page():
    st.title("âš–ï¸ Equity Intelligence")

    lsoa_data = load_lsoa_data()
    lsoa_data['equity_index'] = compute_equity_index(lsoa_data)
    equity_gaps = identify_equity_gaps(lsoa_data)

    # KPIs
    col1, col2, col3 = st.columns(3)
    with col1:
        avg_equity = lsoa_data['equity_index'].mean()
        st.metric("Average Equity Score", f"{avg_equity:.1f}/100")
    with col2:
        gap_count = len(equity_gaps)
        st.metric("Priority Equity Gaps", f"{gap_count:,} LSOAs")
    with col3:
        affected_pop = equity_gaps['population'].sum()
        st.metric("Affected Population", f"{affected_pop:,}")

    # Scatter plot: Deprivation vs. Service
    st.subheader("Service Provision vs. Deprivation")

    fig = px.scatter(
        lsoa_data,
        x='imd_decile',
        y='coverage_score',
        color='equity_index',
        size='population',
        hover_data=['lsoa_name', 'region'],
        title='Coverage Score by Deprivation Decile',
        labels={
            'imd_decile': 'IMD Decile (1=Most Deprived, 10=Least)',
            'coverage_score': 'Service Coverage Score'
        },
        color_continuous_scale='RdYlGn'
    )

    # Add ideal equity line (y = 100 - x*10)
    fig.add_scatter(
        x=[1, 10],
        y=[90, 0],
        mode='lines',
        name='Ideal Equity Line',
        line=dict(dash='dash', color='blue')
    )

    st.plotly_chart(fig, use_container_width=True)

    st.caption("ðŸ’¡ Points below the ideal line indicate under-provision in deprived areas")
```

---

### 4.4 Module 4: Investment Appraisal Engine

**Dashboard Page:** `04_ðŸ’°_Investment_Appraisal.py`

#### 4.4.1 BCR Calculator Implementation

**(Already implemented in `analysis/spatial/utils/bcr_calculator.py` - see previous enhancement)**

**Dashboard Integration:**

```python
# dashboard/pages/04_ðŸ’°_Investment_Appraisal.py

from analysis.spatial.utils.bcr_calculator import BCRCalculator

def render_investment_appraisal_page():
    st.title("ðŸ’° Investment Appraisal Engine")

    st.markdown("""
    **Methodology:** UK Treasury Green Book + DfT TAG 2025
    **Appraisal Period:** 30 years | **Discount Rate:** 3.5%
    """)

    # User inputs
    col1, col2 = st.columns(2)

    with col1:
        investment_amount = st.number_input(
            "Investment Amount (Â£ millions)",
            min_value=1.0,
            max_value=100.0,
            value=10.0,
            step=1.0
        ) * 1_000_000

    with col2:
        num_lsoas = st.slider(
            "Number of LSOAs to Target",
            min_value=5,
            max_value=50,
            value=10
        )

    adoption_rate = st.slider(
        "Projected Adoption Rate (%)",
        min_value=10,
        max_value=50,
        value=25
    ) / 100

    modal_shift = st.slider(
        "Modal Shift from Car (%)",
        min_value=50,
        max_value=90,
        value=70
    ) / 100

    # Calculate BCR
    if st.button("Calculate BCR", type="primary"):
        with st.spinner("Running economic appraisal..."):

            # Select top underserved LSOAs
            lsoa_data = load_lsoa_data()
            underserved = identify_underserved_lsoas(lsoa_data, top_n=num_lsoas)

            # Run BCR calculation
            calculator = BCRCalculator()
            result = calculator.calculate_full_bcr(
                lsoa_data=underserved,
                investment_amount=investment_amount,
                adoption_rate=adoption_rate,
                modal_shift_from_car=modal_shift
            )

            # Display results
            st.success("âœ… Economic Appraisal Complete")

            # Key metrics
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("BCR", f"{result['summary']['bcr']:.2f}")
            with col2:
                npv = result['summary']['npv']
                st.metric("NPV", f"Â£{npv/1_000_000:.1f}M")
            with col3:
                total_cost = result['summary']['total_cost_pv']
                st.metric("Total Cost (PV)", f"Â£{total_cost/1_000_000:.1f}M")
            with col4:
                total_benefit = result['summary']['total_benefits_pv']
                st.metric("Total Benefits (PV)", f"Â£{total_benefit/1_000_000:.1f}M")

            # Recommendation
            rec = result['summary']['recommendation']
            priority = result['summary']['priority']

            if priority == 'HIGH':
                st.success(f"**Recommendation:** {rec}")
            elif priority in ['MEDIUM-HIGH', 'MEDIUM']:
                st.info(f"**Recommendation:** {rec}")
            else:
                st.warning(f"**Recommendation:** {rec}")

            # Benefit breakdown
            st.subheader("Benefit Breakdown (Present Value)")

            benefits_df = pd.DataFrame([
                {'Benefit Type': k.replace('_', ' ').title(),
                 'Value (Â£)': v}
                for k, v in result['benefit_breakdown_pv'].items()
            ])

            fig = px.bar(
                benefits_df,
                x='Benefit Type',
                y='Value (Â£)',
                title='Economic Benefits by Component (30-year PV)',
                color='Value (Â£)',
                color_continuous_scale='Greens'
            )
            st.plotly_chart(fig, use_container_width=True)

            # Target LSOAs
            st.subheader("Target Investment Areas")

            target_display = pd.DataFrame(result['target_lsoas'])
            st.dataframe(target_display, use_container_width=True)
```

---

### 4.5 Module 5: Policy Scenario Intelligence

**Dashboard Page:** `05_ðŸŽ¯_Policy_Scenarios.py`

#### 4.5.1 Scenario Simulator Implementation

**(Already implemented in `analysis/spatial/05_policy_scenario_simulator.py` - see previous enhancement)**

**Dashboard Integration:**

```python
# dashboard/pages/05_ðŸŽ¯_Policy_Scenarios.py

from analysis.spatial.05_policy_scenario_simulator import PolicyScenarioSimulator

def render_policy_scenarios_page():
    st.title("ðŸŽ¯ Policy Scenario Intelligence")

    st.markdown("""
    Test policy interventions before implementation using economic elasticity models.
    """)

    simulator = PolicyScenarioSimulator()

    # Scenario selection
    scenario_type = st.selectbox(
        "Select Policy Scenario",
        ["Fare Cap", "Frequency Increase", "Coverage Expansion", "Combined Package"]
    )

    if scenario_type == "Fare Cap":
        st.subheader("Fare Cap Scenario")

        fare_cap = st.select_slider(
            "Fare Cap Level",
            options=[1.00, 1.50, 2.00, 2.50, 3.00],
            value=2.00,
            format_func=lambda x: f"Â£{x:.2f}"
        )

        if st.button("Run Scenario", type="primary"):
            with st.spinner("Simulating policy impact..."):
                result = simulator.simulate_fare_cap(fare_cap)

                display_scenario_results(result, scenario_type="Fare Cap")

    elif scenario_type == "Frequency Increase":
        st.subheader("Frequency Increase Scenario")

        freq_increase = st.slider(
            "Service Frequency Increase (%)",
            min_value=5,
            max_value=30,
            value=20,
            step=5
        ) / 100

        if st.button("Run Scenario", type="primary"):
            with st.spinner("Simulating policy impact..."):
                result = simulator.simulate_frequency_increase(freq_increase)

                display_scenario_results(result, scenario_type="Frequency")

    # ... similar for other scenarios


def display_scenario_results(result: dict, scenario_type: str):
    """Reusable results display"""

    st.success("âœ… Scenario Analysis Complete")

    # Key metrics
    if 'ridership' in result:
        col1, col2, col3 = st.columns(3)

        with col1:
            ridership_change = result['ridership']['ridership_change_pct']
            st.metric(
                "Ridership Impact",
                f"+{ridership_change:.1f}%",
                delta=f"{result['ridership']['additional_trips']:,.0f} trips/year"
            )

        with col2:
            if 'subsidy' in result:
                subsidy_change = result['subsidy']['additional_subsidy_needed']
                st.metric(
                    "Additional Subsidy",
                    f"Â£{subsidy_change/1_000_000:.0f}M/year"
                )
            elif 'costs' in result:
                net_subsidy = result['costs']['net_subsidy_change']
                st.metric(
                    "Net Subsidy Impact",
                    f"Â£{net_subsidy/1_000_000:.0f}M/year"
                )

        with col3:
            bcr_key = 'bcr_10yr' if 'bcr_10yr' in result else 'bcr_30yr'
            bcr_value = result[bcr_key]
            st.metric("BCR", f"{bcr_value:.2f}")

    # Recommendation
    st.info(f"**Recommendation:** {result['recommendation']}")

    # Detailed breakdown (expandable)
    with st.expander("View Detailed Analysis"):
        st.json(result)
```

---

### 4.6 Module 6: Policy Intelligence Assistant (AI Chat)

**Dashboard Component:** Embedded chat interface across all pages

#### 4.6.1 NLP System Architecture

```python
# dashboard/utils/policy_assistant.py

from sentence_transformers import SentenceTransformer, util
import numpy as np
import json

class PolicyIntelligenceAssistant:
    """
    Context-aware policy advisor using semantic search over analytical framework
    """

    def __init__(self, knowledge_base_path: str):
        """
        Initialize assistant with knowledge base

        Knowledge base structure:
        {
            "questions": {
                "Q1": {
                    "question": "Which regions have lowest coverage?",
                    "answer": "Analysis shows...",
                    "data_source": "lsoa_metrics.parquet",
                    "methodology": "Coverage score calculation..."
                },
                ...
            },
            "capabilities": {
                "BCR Analysis": {
                    "description": "Calculate benefit-cost ratios...",
                    "methodology": "UK Treasury Green Book...",
                    "example": "What is the BCR for Â£10M investment?"
                },
                ...
            },
            "gaps_filled": {
                "Real-time data": "Unlike consulting reports...",
                ...
            }
        }
        """

        # Load knowledge base
        with open(knowledge_base_path, 'r') as f:
            self.knowledge_base = json.load(f)

        # Initialize embedding model
        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

        # Pre-compute embeddings for all knowledge entries
        self._build_knowledge_embeddings()

    def _build_knowledge_embeddings(self):
        """Pre-compute embeddings for fast retrieval"""

        # Flatten knowledge base into searchable entries
        self.knowledge_entries = []
        self.knowledge_metadata = []

        # Add questions
        for q_id, q_data in self.knowledge_base['questions'].items():
            self.knowledge_entries.append(q_data['question'])
            self.knowledge_metadata.append({
                'type': 'question',
                'id': q_id,
                'data': q_data
            })

        # Add capabilities
        for cap_name, cap_data in self.knowledge_base['capabilities'].items():
            self.knowledge_entries.append(
                f"{cap_name}: {cap_data['description']}"
            )
            self.knowledge_metadata.append({
                'type': 'capability',
                'name': cap_name,
                'data': cap_data
            })

        # Add gap explanations
        for gap_name, gap_desc in self.knowledge_base['gaps_filled'].items():
            self.knowledge_entries.append(
                f"How we address {gap_name}: {gap_desc}"
            )
            self.knowledge_metadata.append({
                'type': 'gap',
                'name': gap_name,
                'data': {'description': gap_desc}
            })

        # Compute embeddings
        self.knowledge_embeddings = self.model.encode(
            self.knowledge_entries,
            convert_to_tensor=True,
            show_progress_bar=False
        )

    def answer_question(self, user_query: str, top_k: int = 3) -> dict:
        """
        Answer user question using semantic search

        Returns:
            {
                'answer': str,
                'confidence': float,
                'sources': list,
                'follow_up_suggestions': list
            }
        """

        # Encode user query
        query_embedding = self.model.encode(user_query, convert_to_tensor=True)

        # Compute similarities
        similarities = util.cos_sim(query_embedding, self.knowledge_embeddings)[0]

        # Get top-k matches
        top_indices = similarities.argsort(descending=True)[:top_k]
        top_scores = similarities[top_indices]

        # Best match
        best_idx = top_indices[0].item()
        best_score = top_scores[0].item()
        best_match = self.knowledge_metadata[best_idx]

        # Construct answer
        if best_match['type'] == 'question':
            answer = best_match['data']['answer']
            methodology = best_match['data'].get('methodology', '')
            data_source = best_match['data'].get('data_source', '')

            full_answer = f"{answer}\n\n"
            if methodology:
                full_answer += f"**Methodology:** {methodology}\n\n"
            if data_source:
                full_answer += f"**Data Source:** {data_source}"

        elif best_match['type'] == 'capability':
            cap_data = best_match['data']
            full_answer = f"**{best_match['name']}**\n\n"
            full_answer += f"{cap_data['description']}\n\n"
            full_answer += f"**Methodology:** {cap_data['methodology']}\n\n"
            if 'example' in cap_data:
                full_answer += f"**Example:** {cap_data['example']}"

        else:  # gap
            full_answer = best_match['data']['description']

        # Related suggestions
        related_indices = top_indices[1:].tolist()
        follow_up_suggestions = [
            self.knowledge_entries[idx] for idx in related_indices
        ]

        return {
            'answer': full_answer,
            'confidence': best_score,
            'match_type': best_match['type'],
            'sources': [best_match],
            'follow_up_suggestions': follow_up_suggestions
        }
```

#### 4.6.2 Dashboard Integration

```python
# dashboard/app.py (main application file)

import streamlit as st
from dashboard.utils.policy_assistant import PolicyIntelligenceAssistant

# Initialize assistant (session state for persistence)
if 'assistant' not in st.session_state:
    st.session_state.assistant = PolicyIntelligenceAssistant(
        knowledge_base_path='data/knowledge_base.json'
    )

# Sidebar chat interface (available on all pages)
with st.sidebar:
    st.title("ðŸ’¬ Policy Intelligence Assistant")

    st.caption("Ask questions about UK bus transport policy, economic analysis, or platform capabilities.")

    # Chat history
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    # Display chat history
    for message in st.session_state.chat_history:
        with st.chat_message(message['role']):
            st.markdown(message['content'])

    # User input
    user_query = st.chat_input("Ask a question...")

    if user_query:
        # Add user message to history
        st.session_state.chat_history.append({
            'role': 'user',
            'content': user_query
        })

        # Get answer from assistant
        response = st.session_state.assistant.answer_question(user_query)

        # Add assistant response to history
        st.session_state.chat_history.append({
            'role': 'assistant',
            'content': response['answer']
        })

        # Show confidence if low
        if response['confidence'] < 0.7:
            st.session_state.chat_history.append({
                'role': 'assistant',
                'content': f"âš ï¸ *Confidence: {response['confidence']:.0%}. "
                          f"This answer may not be accurate. Consider refining your question.*"
            })

        # Show follow-up suggestions
        if response['follow_up_suggestions']:
            suggestions_text = "**Related topics you might ask about:**\n"
            for i, suggestion in enumerate(response['follow_up_suggestions'][:3], 1):
                suggestions_text += f"{i}. {suggestion}\n"

            st.session_state.chat_history.append({
                'role': 'assistant',
                'content': suggestions_text
            })

        # Rerun to display updated chat
        st.rerun()
```

---

## 5. Data Models & Schemas

### 5.1 Core Data Schema

```python
# data/schemas/lsoa_metrics_schema.py

LSOA_METRICS_SCHEMA = {
    # Geographic identifiers
    'lsoa_code': 'string',              # E.g., 'E01000001'
    'lsoa_name': 'string',              # E.g., 'City of London 001A'
    'region': 'string',                 # 9 UK regions
    'local_authority': 'string',        # LA name
    'latitude': 'float64',              # LSOA centroid
    'longitude': 'float64',
    'area_sq_km': 'float64',

    # Service metrics
    'bus_stops_count': 'int32',         # Number of stops
    'routes_count': 'int32',            # Unique routes serving LSOA
    'unique_operators': 'int32',        # Number of operators
    'avg_trips_per_day': 'float64',     # Average daily trips
    'service_hours_weekday': 'float64', # Hours of operation
    'service_hours_weekend': 'float64',
    'late_night_service': 'bool',       # Service after 11pm

    # Demographics (Census 2021)
    'population': 'int32',
    'population_density': 'float64',    # Per sq km
    'elderly_pct': 'float64',           # % aged 65+
    'youth_pct': 'float64',             # % aged 0-17
    'working_age_pct': 'float64',       # % aged 18-64

    # Deprivation (IMD 2019)
    'imd_score': 'float64',             # 0-100 scale
    'imd_decile': 'int32',              # 1 (most deprived) - 10
    'imd_rank': 'int32',                # National rank (1 = most deprived)

    # Economic indicators
    'unemployment_rate': 'float64',     # % (NOMIS 2024)
    'median_income': 'float64',         # Â£/year (modelled)
    'business_count': 'int32',          # Number of businesses
    'employment_accessibility': 'float64',  # Index 0-100

    # Transport-related
    'car_ownership_rate': 'float64',    # Households with cars
    'school_count': 'int32',            # Schools in LSOA
    'hospital_distance_km': 'float64',  # Distance to nearest hospital

    # Derived metrics
    'stops_per_1k_pop': 'float64',      # Calculated
    'routes_per_100k_pop': 'float64',
    'coverage_score': 'float64',        # 0-100 composite index
    'equity_index': 'float64',          # 0-100 equity score
    'accessibility_score': 'float64',   # 0-100 overall accessibility

    # ML predictions
    'underserved_score': 'float64',     # Anomaly detector output
    'service_gap_probability': 'float64',  # ML classification probability
    'predicted_demand_growth': 'float64',  # % growth forecast
}
```

### 5.2 Pre-Computed Insights Schema

```json
// data/precomputed_insights.json

{
  "metadata": {
    "generated_date": "2025-10-29",
    "data_version": "Oct_2025",
    "total_lsoas": 7696
  },

  "coverage_analysis": {
    "national": {
      "total_stops": 400000,
      "total_routes": 35000,
      "avg_stops_per_1k_pop": 7.2,
      "service_gap_lsoas": 769
    },
    "by_region": {
      "London": {...},
      "South East": {...}
    }
  },

  "equity_analysis": {
    "national_equity_score": 67.5,
    "equity_gaps": [
      {
        "lsoa_code": "E01000123",
        "lsoa_name": "...",
        "equity_score": 23.4,
        "population": 1800,
        "imd_decile": 1
      }
    ]
  },

  "ml_insights": {
    "route_clusters": {
      "total_clusters": 127,
      "overlap_opportunities": 43
    },
    "service_gap_predictions": {
      "high_risk_lsoas": 234,
      "emerging_gaps": 89
    }
  },

  "economic_analysis": {
    "bcr_scenarios": {
      "10M_investment_top_10_lsoas": {
        "bcr": 2.45,
        "npv": 8250000,
        "target_lsoas": [...]
      }
    },
    "gdp_multipliers": {
      "national_average": 2.15,
      "by_region": {...}
    }
  }
}
```

---

## 6. Machine Learning Workflow

### 6.1 Training Pipeline

```python
# scripts/train_all_models.py

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from analysis.spatial.02_train_ml_models import (
    train_route_clustering_model,
    train_service_gap_detector,
    train_coverage_predictor
)

def main():
    """
    Complete ML training pipeline

    Run this monthly after data refresh
    """

    print("=" * 80)
    print("ML MODEL TRAINING PIPELINE")
    print("=" * 80)

    # Load processed data
    routes_data = pd.read_parquet('data/processed/routes_processed.parquet')
    lsoa_data = pd.read_parquet('data/processed/lsoa_metrics.parquet')

    # Model 1: Route Clustering
    print("\n1. Training Route Clustering Model...")
    route_clusters = train_route_clustering_model(routes_data)
    print("âœ… Route clustering complete")

    # Model 2: Service Gap Detection (Anomaly Detection)
    print("\n2. Training Service Gap Detector...")
    gap_detector = train_service_gap_detector(lsoa_data)
    print("âœ… Service gap detector trained")

    # Model 3: Coverage Prediction (Supervised Learning)
    print("\n3. Training Coverage Predictor...")
    coverage_model = train_coverage_predictor(lsoa_data)
    print("âœ… Coverage predictor trained")

    print("\n" + "=" * 80)
    print("âœ… ALL MODELS TRAINED SUCCESSFULLY")
    print("=" * 80)

    # Save metadata
    model_metadata = {
        'training_date': datetime.now().isoformat(),
        'data_version': 'Oct_2025',
        'models': {
            'route_clustering': {
                'algorithm': 'SentenceTransformers + HDBSCAN',
                'num_clusters': len(route_clusters['cluster_summary']),
                'model_path': 'models/route_clusters.pkl'
            },
            'service_gap_detection': {
                'algorithm': 'Isolation Forest',
                'contamination': 0.10,
                'model_path': 'models/gap_detector.pkl'
            },
            'coverage_prediction': {
                'algorithm': 'Random Forest',
                'n_estimators': 100,
                'model_path': 'models/coverage_predictor.pkl'
            }
        }
    }

    with open('models/model_metadata.json', 'w') as f:
        json.dump(model_metadata, f, indent=2)

if __name__ == '__main__':
    main()
```

### 6.2 Service Gap Detection (Isolation Forest)

```python
# analysis/spatial/02_train_ml_models.py

from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

def train_service_gap_detector(lsoa_data: pd.DataFrame):
    """
    Train anomaly detection model to identify unexpected under-provision

    Features:
        - Stops per capita
        - Routes per capita
        - Population density
        - Deprivation score (should correlate with high service)
        - Unemployment rate

    Output:
        - underserved_score: -1 (anomaly) to 1 (normal)
    """

    # Feature engineering
    features = pd.DataFrame({
        'stops_per_1k_pop': lsoa_data['bus_stops_count'] / lsoa_data['population'] * 1000,
        'routes_per_100k_pop': lsoa_data['routes_count'] / lsoa_data['population'] * 100000,
        'population_density': lsoa_data['population'] / lsoa_data['area_sq_km'],
        'deprivation_score': 10 - lsoa_data['imd_decile'],  # Invert: high = high need
        'unemployment_rate': lsoa_data['unemployment_rate']
    })

    # Handle missing values
    features = features.fillna(features.median())

    # Standardize features
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    # Train Isolation Forest
    model = IsolationForest(
        contamination=0.10,  # Expect 10% of LSOAs to be underserved
        random_state=42,
        n_estimators=100
    )

    anomaly_labels = model.fit_predict(features_scaled)
    anomaly_scores = model.score_samples(features_scaled)

    # Add predictions to data
    lsoa_data['underserved_label'] = anomaly_labels  # -1 = underserved, 1 = normal
    lsoa_data['underserved_score'] = anomaly_scores  # Lower = more anomalous

    # Identify underserved LSOAs
    underserved_lsoas = lsoa_data[lsoa_data['underserved_label'] == -1].copy()
    underserved_lsoas = underserved_lsoas.sort_values('underserved_score')

    # Save model
    model_artifacts = {
        'model': model,
        'scaler': scaler,
        'feature_names': features.columns.tolist(),
        'underserved_lsoas': underserved_lsoas
    }

    with open('models/gap_detector.pkl', 'wb') as f:
        pickle.dump(model_artifacts, f)

    print(f"   Underserved LSOAs detected: {len(underserved_lsoas)}")
    print(f"   Affected population: {underserved_lsoas['population'].sum():,}")

    return model_artifacts
```

---

## 7. API and Integration Layer

### 7.1 Data Loader Module

```python
# dashboard/utils/data_loader.py

import duckdb
import pandas as pd
import streamlit as st
from pathlib import Path

@st.cache_resource
def init_duckdb_connection():
    """
    Initialize DuckDB connection with LSOA data loaded

    Cached at application level (persists across reruns)
    """

    conn = duckdb.connect(database=':memory:')

    # Load LSOA metrics
    conn.execute("""
        CREATE TABLE lsoa_metrics AS
        SELECT * FROM read_parquet('data/lsoa_metrics.parquet')
    """)

    # Create indexes for fast queries
    conn.execute("""
        CREATE INDEX idx_region ON lsoa_metrics(region)
    """)
    conn.execute("""
        CREATE INDEX idx_imd_decile ON lsoa_metrics(imd_decile)
    """)

    print("âœ… DuckDB initialized with LSOA data")

    return conn


@st.cache_data(ttl=3600)  # Cache for 1 hour
def load_lsoa_data(region: str = None, imd_decile_range: tuple = None):
    """
    Load LSOA data with optional filters

    Args:
        region: Filter by UK region (None = all regions)
        imd_decile_range: Tuple (min, max) for IMD decile filter

    Returns:
        pd.DataFrame with LSOA metrics
    """

    conn = init_duckdb_connection()

    query = "SELECT * FROM lsoa_metrics WHERE 1=1"

    if region:
        query += f" AND region = '{region}'"

    if imd_decile_range:
        min_decile, max_decile = imd_decile_range
        query += f" AND imd_decile BETWEEN {min_decile} AND {max_decile}"

    result = conn.execute(query).fetchdf()

    return result


@st.cache_data(ttl=86400)  # Cache for 24 hours
def load_precomputed_insights():
    """Load pre-computed analysis results"""

    with open('data/precomputed_insights.json', 'r') as f:
        return json.load(f)
```

### 7.2 ML Model Loader

```python
# dashboard/utils/ml_loader.py

import pickle
import streamlit as st

@st.cache_resource
def load_route_clusters():
    """Lazy load route clustering model"""
    with open('models/route_clusters.pkl', 'rb') as f:
        return pickle.load(f)

@st.cache_resource
def load_gap_detector():
    """Lazy load service gap detector"""
    with open('models/gap_detector.pkl', 'rb') as f:
        return pickle.load(f)

@st.cache_resource
def load_policy_assistant():
    """Lazy load policy intelligence assistant"""
    from dashboard.utils.policy_assistant import PolicyIntelligenceAssistant
    return PolicyIntelligenceAssistant('data/knowledge_base.json')
```

---

## 8. Dashboard Front-End Implementation (Streamlit)

### 8.1 Application Structure

```
dashboard/
â”œâ”€â”€ app.py                          # Main entry point
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 01_ðŸ—ºï¸_Service_Coverage.py
â”‚   â”œâ”€â”€ 02_ðŸšŒ_Network_Optimization.py
â”‚   â”œâ”€â”€ 03_âš–ï¸_Equity_Intelligence.py
â”‚   â”œâ”€â”€ 04_ðŸ’°_Investment_Appraisal.py
â”‚   â”œâ”€â”€ 05_ðŸŽ¯_Policy_Scenarios.py
â”‚   â””â”€â”€ 06_ðŸ“Š_Performance_Intelligence.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_loader.py              # Data access layer
â”‚   â”œâ”€â”€ ml_loader.py                # Model loading utilities
â”‚   â”œâ”€â”€ policy_assistant.py         # AI chat system
â”‚   â””â”€â”€ viz_components.py           # Reusable visualizations
â””â”€â”€ config/
    â””â”€â”€ streamlit_config.toml       # Streamlit configuration
```

### 8.2 Main Application File

```python
# dashboard/app.py

import streamlit as st
from utils.data_loader import init_duckdb_connection, load_precomputed_insights
from utils.policy_assistant import PolicyIntelligenceAssistant

# Page configuration
st.set_page_config(
    page_title="UK Bus Transport Intelligence Platform",
    page_icon="ðŸšŒ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize connections (cached)
init_duckdb_connection()

# Initialize policy assistant (session state)
if 'assistant' not in st.session_state:
    st.session_state.assistant = PolicyIntelligenceAssistant(
        knowledge_base_path='data/knowledge_base.json'
    )
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# Title
st.title("ðŸšŒ UK Bus Transport Intelligence Platform")
st.markdown("""
A strategic analytics solution for evidence-based transport policy.

**Navigate using the sidebar** to explore different intelligence modules.
""")

# Key metrics overview (home page)
insights = load_precomputed_insights()

col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric(
        "Total Bus Stops",
        f"{insights['coverage_analysis']['national']['total_stops']:,}"
    )

with col2:
    st.metric(
        "LSOAs Analyzed",
        f"{insights['metadata']['total_lsoas']:,}"
    )

with col3:
    equity_score = insights['equity_analysis']['national_equity_score']
    st.metric(
        "National Equity Score",
        f"{equity_score:.1f}/100"
    )

with col4:
    gap_count = insights['coverage_analysis']['national']['service_gap_lsoas']
    st.metric(
        "Service Gap Areas",
        f"{gap_count:,}"
    )

# Quick links
st.subheader("Intelligence Modules")

col1, col2, col3 = st.columns(3)

with col1:
    st.page_link("pages/01_ðŸ—ºï¸_Service_Coverage.py", label="ðŸ—ºï¸ Service Coverage", use_container_width=True)
    st.page_link("pages/02_ðŸšŒ_Network_Optimization.py", label="ðŸšŒ Network Optimization", use_container_width=True)

with col2:
    st.page_link("pages/03_âš–ï¸_Equity_Intelligence.py", label="âš–ï¸ Equity Intelligence", use_container_width=True)
    st.page_link("pages/04_ðŸ’°_Investment_Appraisal.py", label="ðŸ’° Investment Appraisal", use_container_width=True)

with col3:
    st.page_link("pages/05_ðŸŽ¯_Policy_Scenarios.py", label="ðŸŽ¯ Policy Scenarios", use_container_width=True)
    st.page_link("pages/06_ðŸ“Š_Performance_Intelligence.py", label="ðŸ“Š Performance Intelligence", use_container_width=True)

# Sidebar: Policy Assistant (always visible)
with st.sidebar:
    st.title("ðŸ’¬ Policy Intelligence Assistant")
    st.caption("Ask questions about UK bus transport analytics")

    # Display chat
    for msg in st.session_state.chat_history[-10:]:  # Last 10 messages
        with st.chat_message(msg['role']):
            st.markdown(msg['content'])

    # User input
    if prompt := st.chat_input("Ask a question..."):
        st.session_state.chat_history.append({'role': 'user', 'content': prompt})

        response = st.session_state.assistant.answer_question(prompt)
        st.session_state.chat_history.append({'role': 'assistant', 'content': response['answer']})

        st.rerun()
```

### 8.3 Streamlit Configuration

```toml
# dashboard/config/streamlit_config.toml

[theme]
primaryColor = "#1f77b4"
backgroundColor = "#ffffff"
secondaryBackgroundColor = "#f0f2f6"
textColor = "#262730"
font = "sans serif"

[server]
headless = true
port = 8501
enableCORS = false
enableXsrfProtection = true
maxUploadSize = 200

[browser]
gatherUsageStats = false
```

---

## 9. Deployment Plan (Cloud / CI/CD)

### 9.1 Hugging Face Spaces Deployment

**Directory Structure for Deployment:**

```
deployment/
â”œâ”€â”€ app.py                      # Streamlit app (copied from dashboard/)
â”œâ”€â”€ pages/                      # All dashboard pages
â”‚   â””â”€â”€ ...
â”œâ”€â”€ utils/                      # Utilities
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                       # Optimized data (<1GB)
â”‚   â”œâ”€â”€ lsoa_metrics.parquet    (~50MB)
â”‚   â”œâ”€â”€ precomputed_insights.json (~20MB)
â”‚   â”œâ”€â”€ knowledge_base.json     (~5MB)
â”‚   â””â”€â”€ geojson/                (~50MB)
â”œâ”€â”€ models/                     # ML models
â”‚   â”œâ”€â”€ route_clusters.pkl      (~100MB)
â”‚   â”œâ”€â”€ gap_detector.pkl        (~50MB)
â”‚   â””â”€â”€ model_metadata.json
â”œâ”€â”€ analysis/                   # Analysis modules (utils only)
â”‚   â””â”€â”€ spatial/utils/
â”‚       â”œâ”€â”€ bcr_calculator.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Platform documentation
â””â”€â”€ .gitignore
```

**requirements.txt:**

```txt
streamlit==1.28.1
pandas==2.1.3
numpy==1.26.2
plotly==5.17.0
folium==0.15.0
streamlit-folium==0.15.0
duckdb==0.9.2
pyarrow==14.0.1
scikit-learn==1.3.2
sentence-transformers==2.2.2
hdbscan==0.8.33
```

### 9.2 Deployment Script

```bash
#!/bin/bash
# scripts/deploy_to_huggingface.sh

echo "============================================"
echo "DEPLOYING TO HUGGING FACE SPACES"
echo "============================================"

# Step 1: Create deployment directory
echo "1. Creating deployment directory..."
rm -rf deployment/
mkdir -p deployment/data deployment/models deployment/pages deployment/utils deployment/analysis/spatial/utils

# Step 2: Copy dashboard files
echo "2. Copying dashboard files..."
cp dashboard/app.py deployment/
cp -r dashboard/pages/* deployment/pages/
cp -r dashboard/utils/* deployment/utils/
cp -r dashboard/config deployment/

# Step 3: Copy analysis utilities (BCR calculator, etc.)
echo "3. Copying analysis utilities..."
cp analysis/spatial/utils/bcr_calculator.py deployment/analysis/spatial/utils/
cp analysis/spatial/05_policy_scenario_simulator.py deployment/analysis/spatial/
cp analysis/spatial/04_economic_impact_modeling.py deployment/analysis/spatial/

# Step 4: Copy optimized data
echo "4. Copying optimized data..."
cp data/deployment/lsoa_metrics.parquet deployment/data/
cp data/deployment/precomputed_insights.json deployment/data/
cp data/deployment/knowledge_base.json deployment/data/
cp -r data/geojson deployment/data/

# Step 5: Copy ML models
echo "5. Copying ML models..."
cp models/*.pkl deployment/models/
cp models/model_metadata.json deployment/models/

# Step 6: Create requirements.txt
echo "6. Creating requirements.txt..."
cat > deployment/requirements.txt << EOF
streamlit==1.28.1
pandas==2.1.3
numpy==1.26.2
plotly==5.17.0
folium==0.15.0
streamlit-folium==0.15.0
duckdb==0.9.2
pyarrow==14.0.1
scikit-learn==1.3.2
sentence-transformers==2.2.2
hdbscan==0.8.33
EOF

# Step 7: Create README
echo "7. Creating README.md..."
cat > deployment/README.md << EOF
# UK Bus Transport Intelligence Platform

Strategic analytics platform for evidence-based transport policy.

## Features

- Real-time service coverage analysis
- AI-powered network optimization
- Equity intelligence across socio-economic dimensions
- Economic appraisal (BCR, GDP, jobs, carbon)
- Policy scenario simulation
- Intelligent policy advisor (AI chat)

## Data Sources

- Bus Open Data Service (BODS)
- ONS Census & Mid-Year Estimates
- NOMIS Labour Market Statistics
- IMD Deprivation Data
- BEIS Carbon Factors

## Methodology

All economic analysis follows UK Treasury Green Book and DfT TAG 2025 standards.

---

**Platform Status:** Production-ready | **Last Updated:** October 2025
EOF

# Step 8: Check deployment size
echo "8. Checking deployment size..."
du -sh deployment/
echo ""
echo "âœ… Deployment package created in deployment/"
echo ""
echo "NEXT STEPS:"
echo "1. cd deployment/"
echo "2. git init"
echo "3. git remote add origin https://huggingface.co/spaces/YOUR_USERNAME/uk-bus-analytics"
echo "4. git add ."
echo "5. git commit -m 'Initial deployment'"
echo "6. git push -u origin main"
```

### 9.3 GitHub Actions CI/CD

```yaml
# .github/workflows/deploy.yml

name: Deploy to Hugging Face Spaces

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run tests
        run: |
          pytest tests/ -v

      - name: Build deployment package
        run: |
          bash scripts/deploy_to_huggingface.sh

      - name: Deploy to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          cd deployment
          git config --global user.email "github-actions@github.com"
          git config --global user.name "GitHub Actions"
          git init
          git remote add origin https://huggingface.co/spaces/${{ secrets.HF_USERNAME }}/uk-bus-analytics
          git add .
          git commit -m "Automated deployment from GitHub Actions"
          git push --force origin main
```

---

## 10. Testing and Validation Strategy

### 10.1 Test Structure

```
tests/
â”œâ”€â”€ test_data_pipeline.py       # ETL tests
â”œâ”€â”€ test_ml_models.py           # Model accuracy tests
â”œâ”€â”€ test_bcr_calculator.py      # Economic methodology tests
â”œâ”€â”€ test_policy_simulator.py    # Scenario simulation tests
â”œâ”€â”€ test_dashboard.py           # UI component tests
â””â”€â”€ fixtures/
    â””â”€â”€ sample_data.parquet     # Test data
```

### 10.2 Example Test: BCR Calculator

```python
# tests/test_bcr_calculator.py

import pytest
import pandas as pd
from analysis.spatial.utils.bcr_calculator import BCRCalculator

@pytest.fixture
def sample_lsoa_data():
    """Create sample LSOA data for testing"""
    return pd.DataFrame({
        'lsoa_code': ['E01000001', 'E01000002', 'E01000003'],
        'population': [2000, 2500, 1800],
        'imd_decile': [2, 3, 1],
        'unemployment_rate': [8.5, 9.2, 12.1]
    })

def test_bcr_calculator_initialization():
    """Test BCR calculator initializes correctly"""
    calculator = BCRCalculator()
    assert calculator.APPRAISAL_PERIOD == 30
    assert calculator.DISCOUNT_RATE == 0.035

def test_bcr_calculation_returns_valid_result(sample_lsoa_data):
    """Test BCR calculation produces valid output"""
    calculator = BCRCalculator()
    result = calculator.calculate_full_bcr(
        lsoa_data=sample_lsoa_data,
        investment_amount=3_000_000
    )

    # Check structure
    assert 'summary' in result
    assert 'costs' in result
    assert 'benefits' in result

    # Check summary fields
    assert 'bcr' in result['summary']
    assert 'npv' in result['summary']
    assert 'recommendation' in result['summary']

    # Validate BCR range (should be positive for reasonable scenarios)
    assert result['summary']['bcr'] > 0
    assert result['summary']['bcr'] < 10  # Unreasonably high BCRs indicate bugs

def test_bcr_scales_with_investment(sample_lsoa_data):
    """Test BCR scales correctly with investment amount"""
    calculator = BCRCalculator()

    result_1m = calculator.calculate_full_bcr(sample_lsoa_data, 1_000_000)
    result_5m = calculator.calculate_full_bcr(sample_lsoa_data, 5_000_000)

    # Costs should scale linearly
    assert result_5m['costs']['total_cost_pv'] == pytest.approx(
        result_1m['costs']['total_cost_pv'] * 5,
        rel=0.01
    )

def test_bcr_validates_input_data():
    """Test BCR calculator handles invalid input gracefully"""
    calculator = BCRCalculator()

    # Missing required columns
    invalid_data = pd.DataFrame({'lsoa_code': ['E01000001']})

    with pytest.raises(ValueError, match="Missing required columns"):
        calculator.calculate_full_bcr(invalid_data, 1_000_000)
```

### 10.3 Integration Tests

```python
# tests/test_dashboard.py

from streamlit.testing.v1 import AppTest

def test_dashboard_loads_without_errors():
    """Test main dashboard loads successfully"""
    at = AppTest.from_file("dashboard/app.py")
    at.run()

    assert not at.exception

def test_coverage_page_displays_metrics():
    """Test coverage page renders key metrics"""
    at = AppTest.from_file("dashboard/pages/01_ðŸ—ºï¸_Service_Coverage.py")
    at.run()

    # Check metrics are displayed
    assert len(at.metric) >= 4

    # Check map is rendered
    assert len(at.iframe) >= 1  # Folium map

def test_policy_assistant_responds_to_queries():
    """Test AI assistant provides responses"""
    at = AppTest.from_file("dashboard/app.py")
    at.run()

    # Simulate user question
    at.chat_input[0].set_value("Which regions have lowest coverage?").run()

    # Check response is added to chat
    assert len(at.chat_message) > 0
```

---

## 11. Scalability, Monitoring, and Optimization

### 11.1 Performance Optimization

**Caching Strategy:**

```python
# Caching levels in Streamlit

# 1. Resource-level caching (persistent across reruns)
@st.cache_resource
def load_ml_model():
    # Expensive: load once per session
    return pickle.load(open('models/route_clusters.pkl', 'rb'))

# 2. Data-level caching (with TTL)
@st.cache_data(ttl=3600)  # 1 hour
def load_lsoa_data():
    # Moderate expense: cache with expiration
    return pd.read_parquet('data/lsoa_metrics.parquet')

# 3. Session state (user-specific)
if 'user_filters' not in st.session_state:
    st.session_state.user_filters = {}
```

**Query Optimization:**

```python
# Use DuckDB for fast aggregations instead of Pandas

# âŒ Slow (Pandas)
regional_summary = lsoa_data.groupby('region').agg({
    'bus_stops_count': 'sum',
    'population': 'sum'
})

# âœ… Fast (DuckDB)
conn = duckdb.connect(':memory:')
conn.execute("CREATE TABLE lsoa AS SELECT * FROM lsoa_data")
regional_summary = conn.execute("""
    SELECT
        region,
        SUM(bus_stops_count) as total_stops,
        SUM(population) as total_population
    FROM lsoa
    GROUP BY region
""").fetchdf()
```

### 11.2 Monitoring

```python
# dashboard/utils/monitoring.py

import logging
import time
from functools import wraps

# Configure logging
logging.basicConfig(
    filename='logs/dashboard.log',
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

def log_execution_time(func):
    """Decorator to log function execution time"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start

        logger.info(f"{func.__name__} executed in {duration:.2f}s")

        if duration > 5.0:
            logger.warning(f"{func.__name__} took longer than 5s ({duration:.2f}s)")

        return result
    return wrapper

# Usage
@log_execution_time
def compute_coverage_metrics(lsoa_data):
    # ... expensive computation
    pass
```

---

## 12. Version Control & Documentation Standards

### 12.1 Git Workflow

```bash
# Branch structure
main                 # Production-ready code
â”œâ”€â”€ development      # Integration branch
â”‚   â”œâ”€â”€ feature/economic-modeling
â”‚   â”œâ”€â”€ feature/ml-models
â”‚   â””â”€â”€ feature/dashboard-ui
â””â”€â”€ hotfix/bug-123   # Emergency fixes
```

### 12.2 Code Documentation Standards

```python
# All functions must include:

def calculate_equity_index(lsoa_data: pd.DataFrame) -> pd.Series:
    """
    Calculate multi-dimensional equity index for LSOAs

    The equity index measures how well service provision matches need across
    three dimensions: deprivation, age demographics, and car ownership.

    Args:
        lsoa_data: DataFrame with columns:
            - imd_decile: int, 1 (most deprived) to 10 (least deprived)
            - elderly_pct: float, proportion of population aged 65+
            - car_ownership_rate: float, proportion of households with cars
            - coverage_score: float, 0-100 service coverage score

    Returns:
        pd.Series: Equity index (0-100 scale)
            Higher score = better equity
            Score < 50 indicates significant equity gap

    Methodology:
        1. Calculate need scores for each dimension (0-100)
        2. Compare need vs. actual coverage
        3. Weighted composite: deprivation (40%), age (30%), car ownership (30%)

    Example:
        >>> lsoa_data = pd.DataFrame({
        ...     'imd_decile': [2, 8],
        ...     'elderly_pct': [0.20, 0.10],
        ...     'car_ownership_rate': [0.30, 0.80],
        ...     'coverage_score': [70, 60]
        ... })
        >>> equity = calculate_equity_index(lsoa_data)
        >>> print(equity)
        0    75.5  # High deprivation, good coverage = high equity
        1    45.2  # Low need, moderate coverage = lower equity

    See Also:
        - identify_equity_gaps(): Find priority intervention areas
        - UK IMD 2019 Technical Report for deprivation methodology
    """
    # Implementation...
```

---

## 13. Appendix: Complete File Listing

### Production Code Structure

```
uk_bus_analytics/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Raw downloads (not in git)
â”‚   â”œâ”€â”€ processed/                  # Cleaned data
â”‚   â”‚   â”œâ”€â”€ lsoa_metrics.parquet
â”‚   â”‚   â””â”€â”€ routes_processed.parquet
â”‚   â”œâ”€â”€ deployment/                 # Optimized for deployment
â”‚   â”‚   â”œâ”€â”€ lsoa_metrics.parquet
â”‚   â”‚   â”œâ”€â”€ precomputed_insights.json
â”‚   â”‚   â””â”€â”€ knowledge_base.json
â”‚   â””â”€â”€ geojson/
â”‚       â””â”€â”€ lsoa_boundaries.json
â”‚
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ spatial/
â”‚       â”œâ”€â”€ 01_compute_spatial_metrics.py
â”‚       â”œâ”€â”€ 02_train_ml_models.py
â”‚       â”œâ”€â”€ 03_generate_recommendations.py
â”‚       â”œâ”€â”€ 04_economic_impact_modeling.py
â”‚       â””â”€â”€ 05_policy_scenario_simulator.py
â”‚       â””â”€â”€ utils/
â”‚           â””â”€â”€ bcr_calculator.py
â”‚
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ 01_ðŸ—ºï¸_Service_Coverage.py
â”‚   â”‚   â”œâ”€â”€ 02_ðŸšŒ_Network_Optimization.py
â”‚   â”‚   â”œâ”€â”€ 03_âš–ï¸_Equity_Intelligence.py
â”‚   â”‚   â”œâ”€â”€ 04_ðŸ’°_Investment_Appraisal.py
â”‚   â”‚   â”œâ”€â”€ 05_ðŸŽ¯_Policy_Scenarios.py
â”‚   â”‚   â””â”€â”€ 06_ðŸ“Š_Performance_Intelligence.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â”œâ”€â”€ ml_loader.py
â”‚   â”‚   â”œâ”€â”€ policy_assistant.py
â”‚   â”‚   â””â”€â”€ viz_components.py
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ streamlit_config.toml
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ route_clusters.pkl
â”‚   â”œâ”€â”€ gap_detector.pkl
â”‚   â”œâ”€â”€ coverage_predictor.pkl
â”‚   â””â”€â”€ model_metadata.json
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_all_models.py
â”‚   â”œâ”€â”€ deploy_to_huggingface.sh
â”‚   â””â”€â”€ prepare_deployment_data.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_pipeline.py
â”‚   â”œâ”€â”€ test_ml_models.py
â”‚   â”œâ”€â”€ test_bcr_calculator.py
â”‚   â”œâ”€â”€ test_policy_simulator.py
â”‚   â””â”€â”€ test_dashboard.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PROJECT_PLAN_CONSULTING_GRADE.md
â”‚   â”œâ”€â”€ TECHNICAL_IMPLEMENTATION_PLAN.md
â”‚   â””â”€â”€ API_DOCUMENTATION.md
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

**Document Status:** Implementation-Ready
**Next Steps:** Begin module-by-module implementation following this specification
**Review Cycle:** Monthly updates as project evolves

---

*This technical implementation plan provides the complete engineering blueprint for building the UK Bus Transport Intelligence Platform. All code examples are production-ready and follow industry best practices.*
