name: UK Bus Analytics - Automated Data Pipeline

on:
  schedule:
    - cron: '0 2 * * 1'    # Weekly Monday 2 AM (Transport data)
    - cron: '0 3 1 * *'    # Monthly 1st at 3 AM (Demographics)
  workflow_dispatch:        # Manual trigger for testing
    inputs:
      data_type:
        description: 'Data type to download'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - transport
        - demographic
      regions:
        description: 'Regions to process (comma-separated or "all")'
        required: false
        default: 'all'

jobs:
  automated_data_ingestion:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create environment file
      run: |
        echo "BODS_API_KEY=${{ secrets.BODS_API_KEY }}" > .env

    - name: Run Transport Data Ingestion
      if: github.event.inputs.data_type == 'transport' || github.event.inputs.data_type == 'all' || github.event_name == 'schedule'
      run: |
        python data_pipeline/01_data_ingestion.py --data-type=transport
        echo "Transport data ingestion completed"

    - name: Run Demographic Data Ingestion
      if: github.event.inputs.data_type == 'demographic' || github.event.inputs.data_type == 'all' || github.event_name == 'schedule'
      run: |
        python data_pipeline/01_data_ingestion.py --data-type=demographic
        echo "Demographic data ingestion completed"

    - name: Run Data Processing Pipeline
      run: |
        python data_pipeline/02_data_processing.py
        echo "Data processing completed"

    - name: Run Data Validation
      run: |
        python data_pipeline/03_data_validation.py
        echo "Data validation completed"

    - name: Generate Ingestion Report
      run: |
        echo "## ðŸš€ Automated Data Pipeline Report" > pipeline_report.md
        echo "**Date:** $(date)" >> pipeline_report.md
        echo "" >> pipeline_report.md
        echo "### Transport Data Status" >> pipeline_report.md
        if [ -f "data_pipeline/raw/ingestion_report.json" ]; then
          python -c "
import json
with open('data_pipeline/raw/ingestion_report.json', 'r') as f:
    data = json.load(f)
    regions = data.get('regional_breakdown', {})
    print(f'- **Regions processed:** {len(regions)}')
    total_datasets = sum(r.get('datasets_downloaded', 0) for r in regions.values())
    print(f'- **Total datasets downloaded:** {total_datasets}')
    failed = sum(r.get('datasets_failed', 0) for r in regions.values())
    print(f'- **Failed downloads:** {failed}')
" >> pipeline_report.md
        fi
        echo "" >> pipeline_report.md
        echo "### Demographic Data Status" >> pipeline_report.md
        python -c "
import json
try:
    with open('data_pipeline/raw/ingestion_report.json', 'r') as f:
        data = json.load(f)
        demo_datasets = data.get('demographic_datasets', [])
        print(f'- **Demographic datasets:** {len(demo_datasets)}')
        errors = data.get('errors', [])
        print(f'- **Errors:** {len(errors)}')
        warnings = data.get('warnings', [])
        print(f'- **Warnings:** {len(warnings)}')
except: pass
" >> pipeline_report.md

    - name: Upload pipeline artifacts
      uses: actions/upload-artifact@v3
      with:
        name: uk-bus-analytics-data-${{ github.run_number }}
        path: |
          data_pipeline/raw/ingestion_report.json
          data_pipeline/processed/
          logs/
          pipeline_report.md
        retention-days: 30

    - name: Commit updated data (if changes)
      if: github.event_name == 'schedule'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"

        # Check if there are changes to commit
        if [ -n "$(git status --porcelain)" ]; then
          git add data_pipeline/raw/ingestion_report.json
          git add data_pipeline/processed/
          git commit -m "ðŸ¤– Automated data update - $(date '+%Y-%m-%d %H:%M')" || exit 0
          git push || echo "Push failed - likely due to permissions"
        else
          echo "No changes to commit"
        fi

    - name: Create issue on failure
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'ðŸš¨ Automated Data Pipeline Failed',
            body: `
            ## Pipeline Failure Report

            **Workflow:** ${context.workflow}
            **Run ID:** ${context.runId}
            **Date:** ${new Date().toISOString()}
            **Trigger:** ${context.eventName}

            ### Failure Details
            - **Job:** ${context.job}
            - **Action:** ${context.action}

            ### Next Steps
            1. Check the [workflow logs](${context.payload.repository.html_url}/actions/runs/${context.runId})
            2. Verify API keys and credentials
            3. Check data source availability
            4. Review error messages in the logs

            ### Automated Actions
            - [ ] Check BODS API status
            - [ ] Verify NOMIS API access
            - [ ] Review data source configurations
            - [ ] Test manual pipeline execution

            This issue was created automatically by the failed workflow.
            `,
            labels: ['automation', 'bug', 'pipeline-failure']
          })